{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles I've been reading for NLP\n",
    "\n",
    "\n",
    "https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "\n",
    "https://codereview.stackexchange.com/questions/181152/identify-and-extract-urls-from-text-corpus\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "https://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/\n",
    "\n",
    "https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n",
    "https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "\n",
    "https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184\n",
    "\n",
    "http://www.nltk.org/book/ch06.html\n",
    "\n",
    "https://medium.freecodecamp.org/basic-data-analysis-on-twitter-with-python-251c2a85062e \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import langdetect\n",
    "#either pip install langdetect or conda install -c conda-forge langdetect\n",
    "from langdetect import detect\n",
    "from pandas.plotting import scatter_matrix\n",
    "import re\n",
    "#conda install -c conda-forge textblob \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import html\n",
    "from html.parser import HTMLParser\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in all the tweets from geniuine accounts\n",
    "tweets = pd.read_csv('~/Downloads/datasets_full/datasets_full.csv/genuine_accounts.csv/tweets.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#schema from tweets.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 2839362 entries, 0 to 2839361\n",
    "Data columns (total 25 columns):\n",
    "id                         object\n",
    "text                       object\n",
    "source                     object\n",
    "user_id                    float64\n",
    "truncated                  float64\n",
    "in_reply_to_status_id      float64\n",
    "in_reply_to_user_id        float64\n",
    "in_reply_to_screen_name    object\n",
    "retweeted_status_id        float64\n",
    "geo                        float64\n",
    "place                      object\n",
    "contributors               float64\n",
    "retweet_count              float64\n",
    "reply_count                float64\n",
    "favorite_count             float64\n",
    "favorited                  float64\n",
    "retweeted                  float64\n",
    "possibly_sensitive         float64\n",
    "num_hashtags               float64\n",
    "num_urls                   float64\n",
    "num_mentions               float64\n",
    "created_at                 object\n",
    "timestamp                  object\n",
    "crawled_at                 object\n",
    "updated                    object\n",
    "dtypes: float64(16), object(9)\n",
    "memory usage: 541.6+ MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_detect_en(df, text_col):\n",
    "    \n",
    "    '''Input is a dataframe (df) and name of the column (text_col) to check for english \n",
    "    This function creates a new Boolean column called \"en_flag\"\n",
    "    \"en_flag\" is True if the text_col column is detected as \"en\" \n",
    "    Dataframe with the new column is returned.\n",
    "    '''\n",
    "    \n",
    "    def detect_en(x):\n",
    "        #assumes you have langdetect imported\n",
    "        flag = False\n",
    "        lang = detect(text)\n",
    "        if lang=='en':\n",
    "            flag = True\n",
    "        return flag\n",
    "    \n",
    "    df[text_col] = df[text_col].astype(str)\n",
    "    df['en_flag'] = df.loc[text_col].apply(lambda x: detect_en(x))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "class MLStripper(HTMLParser):\n",
    "    #https://docs.python.org/3/library/html.parser.html\n",
    "    #https://stackoverflow.com/questions/11061058/using-htmlparser-in-python-3-2\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "       \n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def remove_swords(text):\n",
    "    #https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "    stop_words = set(stopwords.words('english'))   \n",
    "    word_tokens = word_tokenize(text)  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
    "    return filtered_sentence\n",
    "\n",
    "def get_tweet_sentiment(tweet): \n",
    "        ''' \n",
    "        https://medium.freecodecamp.org/basic-data-analysis-on-twitter-with-python-251c2a85062e \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(tweet) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "\n",
    "def lemmatize(text):\n",
    "    #https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "    text_out = []\n",
    "    def get_lemma(word):\n",
    "        lemma = wn.morphy(word)\n",
    "        if lemma is None:\n",
    "            return word\n",
    "        else:\n",
    "            return lemma\n",
    "    \n",
    "    for word in text:\n",
    "        lword = get_lemma(word)\n",
    "        text_out.append(lword)\n",
    "    return text_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blah_ids['text'] = blah_ids['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_clean(text):\n",
    "    #get rid of #retweets RT @[\\S]+  mentions @[\\S]+ urls http:\\S+|https\\S+|www.\\S+ punctuation\n",
    "    text_out = []\n",
    "    result = ''\n",
    "    try:\n",
    "        result = re.sub(r\"RT @[\\S]+: |@[\\S]+|http:\\S+|https\\S+|www.\\S+|[^\\w\\s]\", \"\", text) \n",
    "        \n",
    "         #to lower case\n",
    "        result = result.lower()\n",
    "        \n",
    "        #get rid of url encoding\n",
    "        #https://stackoverflow.com/questions/11061058/using-htmlparser-in-python-3-2\n",
    "        result = strip_tags(result)\n",
    "\n",
    "        #get rid of special ascii characters\n",
    "        result = ''.join([c for c in result if ord(c) < 128])\n",
    "\n",
    "       \n",
    "        #get rid of stopwords\n",
    "        result = remove_swords(result)\n",
    "\n",
    "        #get word roots\n",
    "        result = lemmatize(result)\n",
    "        \n",
    "    except:\n",
    "        text_out = ['Failed']\n",
    "    \n",
    "    return result\n",
    "\n",
    "#get sentiment\n",
    "def sentiment(text):\n",
    "    sentiment = get_tweet_sentiment(text)\n",
    "    return sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df, text_col):\n",
    "    #creates two new features\n",
    "    #word_bag is our bag of words that has been cleaned\n",
    "    #sentiment is the sentiment for the individual tweet\n",
    "    df['word_bag'] = df[text_col].apply(lambda x: nlp_clean(x))\n",
    "    df['sentiment'] = df[text_col].apply(lambda x: sentiment(x))\n",
    "    return(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6396"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This filters for all the tweets from two users (6396 total tweets)  \n",
    "two_ids = tweets.loc[(tweets['user_id'] == 678033) | (tweets['user_id'] == 722623)]\n",
    "len(two_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_clean = clean_tweets(two_ids,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['still',\n",
       " 'part',\n",
       " 'group',\n",
       " 'call',\n",
       " 'end',\n",
       " 'humanize',\n",
       " 'capital',\n",
       " 'lay',\n",
       " 'waste',\n",
       " 'impulse',\n",
       " 'leading']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean['word_bag'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex(levels=[[678033.0, 722623.0], ['negative', 'neutral', 'positive']],\n",
       "           labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n",
       "           names=['user_id', 'sentiment'])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupby('reciept')['prod_name'].count()\n",
    "df = test_clean.groupby(['user_id','sentiment']).count()\n",
    "df.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
