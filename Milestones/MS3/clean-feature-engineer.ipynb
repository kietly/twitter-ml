{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from langdetect import detect\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly accumulate the twitter dataset we need, we decided to use twitter dataset from MIB, http://mib.projects.iit.cnr.it/dataset.html, which hosted by Institute of Infomatics and Telmatics of the Italian National Research council. It has the following data collection.\n",
    "\n",
    "![MIB dataset](image/mib.png)\n",
    "\n",
    "We randomly selected 100 humans from genuine accounts and 100 from traditional spambots #1. The data size is a tunable parameter. The reason for reduce number of accounts is when joining with tweets dataset to get a combine dataset between user account and tweets per account. . If 1 account has 10 tweets on average then when we join them, 100 $*$ 10 tweets = 1000 observations $*$ 2 class (bot/human) = 2000 observations. This is just to limit the training time and get the result in reasonable time if we doing it on Jupyterhub.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Extraction #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "#### Creating user sample discussion ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected a smaller sample of 100 users, bots who meet our following conditions:\n",
    "    1. English tweeters so we can do NLP on tweeted text.\n",
    "    2. Tweeted between 100 to 300 times.\n",
    "    \n",
    "We need to start with tweets.csv first as not every user in users.csv has actual tweets in tweets.csv. One technical problem here is the tweets.csv is nearly 1 GB and pd.read_csv() causes out of memory error when it tried to read the entire file into memory. Instead, we will filter line line by from tweets.csv to avoid out of memory error. Even with a relatively clean dataset, there is still '', and 'NA' in user_id column that still need to filter out from tweets.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "def create_100_sample_users(in_tweets_file, in_users_file, out_tweets_file, out_users_file):\n",
    "    # in_tweets_file - original input tweets file location\n",
    "    # in_users_file - original input users file location\n",
    "    # out_tweets_file - sample tweets from 100 selected users\n",
    "    # out_users_file - sample 100 selected users\n",
    "    \n",
    "    # get users_100.csv\n",
    "    tweets_user_df = pd.read_csv(in_tweets_file, usecols=['user_id'], dtype = {'user_id': float}, na_values=['','NA'], keep_default_na=False)\n",
    "    users_df = pd.read_csv(in_users_file,dtype = {'id': float}, na_values=['','NA'], keep_default_na=False)\n",
    "    tweets_user_count = tweets_user_df.groupby(['user_id']).size().reset_index(name='counts')\n",
    "    tweets_user_count = tweets_user_count[tweets_user_count.counts.between(100,1000, inclusive=True)].sort_values(by=['counts'],ascending=True).set_index('user_id')\n",
    "    selected_users_df = users_df.join(tweets_user_count, on='id',how='inner')\n",
    "    eng_selected_users_df = selected_users_df[selected_users_df.lang == 'en']\n",
    "    eng_selected_users_df = eng_selected_users_df.sort_values(by=['counts'],ascending=True).head(100) \n",
    "    eng_selected_users_df.to_csv(out_users_file)\n",
    "    id_df = eng_selected_users_df[['id']]\n",
    "    \n",
    "    # get the tweets_100.csv\n",
    "    # TODO: make this code run faster\n",
    "    row_count = 0\n",
    "    with open(out_tweets_file, mode='w', encoding='utf-8') as tweets_file:\n",
    "        for row in pd.read_csv(in_tweets_file, dtype = {'user_id': float}, \n",
    "                               na_values=['','NA'],keep_default_na=False, chunksize=1):\n",
    "            \n",
    "            if row['user_id'].isin(id_df['id']).any():\n",
    "                if row_count == 0: # header row\n",
    "                    row.to_csv(tweets_file, mode='a', header=True, encoding='utf-8')\n",
    "                    row_count += 1\n",
    "                else:\n",
    "                    row.to_csv(tweets_file, mode='a', header=False, encoding='utf-8')\n",
    "                    row_count += 1\n",
    "                \n",
    "    print('tweets row count = %d' % row_count )\n",
    "\n",
    "def df_detect_en(df):\n",
    "    \n",
    "    # Input is a dataframe (df) to check for english \n",
    "    # Dataframe with the new \"lang\" column is returned.\n",
    "    # It used 3 columns to determine the user is english or not\n",
    "    # name, descripition, location\n",
    "    \n",
    "    def detect_en(name,desc,loc):\n",
    "        langs = []\n",
    "        try:\n",
    "            if name is not None:\n",
    "                langs.append(detect(name))\n",
    "        except:\n",
    "            langs.append('unk')\n",
    "            \n",
    "        try:\n",
    "            if desc is not None:\n",
    "                langs.append(detect(desc))\n",
    "        except:\n",
    "            langs.append('unk')\n",
    "\n",
    "        try:\n",
    "            if loc is not None:\n",
    "                langs.append(detect(loc))\n",
    "        except:\n",
    "            langs.append('unk')\n",
    "            \n",
    "                \n",
    "        if 'en' in langs:\n",
    "            return 'en'\n",
    "        else:\n",
    "            return 'unk'\n",
    "    \n",
    "    df['lang'] = df[['name','description','location']].apply(lambda x: detect_en(*x),axis=1)\n",
    "    return(df)\n",
    "\n",
    "def create_100_bot_users(in_tweets_file, in_users_file, out_tweets_file, out_users_file):\n",
    "    # in_tweets_file - original input tweets file location\n",
    "    # in_users_file - original input users file location\n",
    "    # out_tweets_file - sample tweets from 100 selected users\n",
    "    # out_users_file - sample 100 selected users\n",
    "    \n",
    "    # get users_100.csv\n",
    "    tweets_user_df = pd.read_csv(in_tweets_file, usecols=['user_id'], dtype = {'user_id': float}, na_values=['','NA'], keep_default_na=False)\n",
    "    users_df = pd.read_csv(in_users_file,dtype = {'id': float}, na_values=['','NA'], keep_default_na=False)\n",
    "    tweets_user_count = tweets_user_df.groupby(['user_id']).size().reset_index(name='counts')\n",
    "    #tweets_user_count = tweets_user_count[tweets_user_count.counts.between(10,1000, inclusive=True)].sort_values(by=['counts'],ascending=True).set_index('user_id')\n",
    "    tweets_user_count = tweets_user_count.sort_values(by=['counts'],ascending=True).set_index('user_id')\n",
    "    selected_users_df = users_df.join(tweets_user_count, on='id',how='inner') \n",
    "    eng_selected_users_df = selected_users_df[selected_users_df.lang == 'en'] # get only english user\n",
    "    \n",
    "    # block this out so we get more tweets. There is no guarantee these english users has any tweets in tweets.csv\n",
    "    eng_selected_users_df = eng_selected_users_df.sort_values(by=['counts'],ascending=True)\n",
    "    eng_selected_users_df.to_csv(out_users_file, mode='w', header=True, encoding='utf-8')\n",
    "    \n",
    "    id_df = eng_selected_users_df[['id','counts']].set_index('id')\n",
    "    \n",
    "    # get the tweets_100.csv\n",
    "    row_count = 0\n",
    "    with open(out_tweets_file, mode='w', encoding='utf-8') as tweets_file:\n",
    "        tweets_df = pd.read_csv(in_tweets_file, dtype = {'user_id': float}\n",
    "                                ,na_values=['','NA'],keep_default_na=False)\n",
    "            \n",
    "        selected_tweets_df = tweets_df.join(id_df, on='user_id', how='inner')\n",
    "        selected_tweets_df.to_csv(out_tweets_file, mode='a', header=True, encoding='utf-8')\n",
    "        \n",
    "        # now join with eng_selected_users to filter the users that actual tweets.\n",
    "        #selected_tweets_count_df = selected_tweets_df.groupby(['user_id']).size().reset_index(name='counts')\n",
    "        #selected_tweets_count_df = selected_tweets_count_df.sort_values(by=['counts'],ascending=True).set_index('user_id')\n",
    "        #selected_tweets_count_df = selected_tweets_count_df.drop('counts',axis=1)\n",
    "        #print(selected_tweets_count_df.shape, eng_selected_users_df.shape)\n",
    "        #eng_user_has_tweets_df = eng_selected_users_df.join(selected_tweets_count_df, on='id', how='inner')\n",
    "        #eng_user_has_tweets_df.to_csv(out_users_file, mode='a', header=True, encoding='utf-8')\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "# warning: this many take +2 hours to finish\n",
    "#create_100_sample_users('data/genuine_accounts.csv/tweets.csv', 'data/genuine_accounts.csv/users.csv','clean/human_tweets_100.csv' ,'clean/human_users_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating sample traditional bots discussion ####\n",
    "\n",
    "The processing of traditional bots is slightly different from processing human tweets. The language field is NULL and we have to use langdectect modules to scan the name, location, and description column to detect if the user is an English Twitter users. We need the tweets to be in English to run NLP, sentimental analysis. etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "bot_users_df = pd.read_csv('data/traditional_spambots_1.csv/users.csv', dtype = {'id': float}, na_values=['','NA'], keep_default_na=False)\n",
    "bot_users_df['lang'] = 'unk'\n",
    "bot_users_df = df_detect_en(bot_users_df)\n",
    "bot_users_df.to_csv('data/traditional_spambots_1.csv/users2.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "create_100_bot_users('data/traditional_spambots_1.csv/tweets.csv', 'data/traditional_spambots_1.csv/users2.csv','clean/bot_tweets_100.csv' ,'clean/bot_users_100.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating social spam bots discussion ####\n",
    "\n",
    "Social spam bots are made themselves look like another human users. We can reuse the code for human sample extraction. The downside is it is much harder to make distinction between them and human users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_100_sample_users('data/social_spambots_2.csv/tweets.csv', 'data/social_spambots_2.csv/users.csv','clean/social_tweets_100.csv' ,'clean/social_users_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Feature Engineering #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create response field discussion ####\n",
    "\n",
    "We need to add response 'user_type' column to sample tweets and users for both bots and humans dataset. The user_type = 1 if human and 0 if it is a bot. We will use this field to stratify when splitting train/test/validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "# add reponse variable to human dataset\n",
    "human_users_df = pd.read_csv('clean/human_users_100.csv')\n",
    "human_users_df = human_users_df.assign(user_type=1)\n",
    "human_tweets_df = pd.read_csv('clean/human_tweets_100.csv')\n",
    "human_tweets_df = human_tweets_df.assign(user_type=1)\n",
    "\n",
    "# add reponse variable to traditional bot dataset\n",
    "bot_users_df = pd.read_csv('clean/bot_users_100.csv')\n",
    "bot_users_df = bot_users_df.assign(user_type=0)\n",
    "bot_tweets_df = pd.read_csv('clean/bot_tweets_100.csv')\n",
    "bot_tweets_df = bot_tweets_df.assign(user_type=0)\n",
    "\n",
    "# add reponse variable to traditional bot dataset\n",
    "social_users_df = pd.read_csv('clean/social_users_100.csv')\n",
    "social_users_df = social_users_df.assign(user_type=0)\n",
    "social_tweets_df = pd.read_csv('clean/social_tweets_100.csv')\n",
    "social_tweets_df = social_tweets_df.assign(user_type=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "human_users_df.to_csv('feature/human_users_100.csv', encoding='utf-8')\n",
    "human_tweets_df.to_csv('feature/human_tweets_100.csv', encoding='utf-8')\n",
    "bot_users_df.to_csv('feature/bot_users_100.csv', encoding='utf-8')\n",
    "bot_tweets_df.to_csv('feature/bot_tweets_100.csv', encoding='utf-8')\n",
    "social_users_df.to_csv('feature/social_users_100.csv', encoding='utf-8')\n",
    "social_tweets_df.to_csv('feature/social_tweets_100.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tweeting rate field ####\n",
    "\n",
    "We create multiindex ['userid','timestamp'] where timestamp is DateTimeIndex object. We can use that indexes to count daily tweet rate of each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "human_tweets_df = pd.read_csv('feature/human_tweets_100.csv')\n",
    "bot_tweets_df = pd.read_csv('feature/bot_tweets_100.csv')\n",
    "social_tweets_df = pd.read_csv('feature/social_tweets_100.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "human_tweets_df['timestamp'] = pd.to_datetime(human_tweets_df.timestamp)\n",
    "bot_tweets_df['timestamp'] = pd.to_datetime(bot_tweets_df.timestamp)\n",
    "social_tweets_df['timestamp'] = pd.to_datetime(social_tweets_df.timestamp)\n",
    "\n",
    "human_tweets_df['timestamp'] = human_tweets_df[\"timestamp\"].apply( lambda human_tweets_df : \n",
    "datetime.datetime(year=human_tweets_df.year, month=human_tweets_df.month, day=human_tweets_df.day))\n",
    "human_tweets_df.set_index(['user_id',\"timestamp\"],inplace=True)\n",
    "\n",
    "bot_tweets_df['timestamp'] = bot_tweets_df[\"timestamp\"].apply( lambda bot_tweets_df : \n",
    "datetime.datetime(year=bot_tweets_df.year, month=bot_tweets_df.month, day=bot_tweets_df.day))\n",
    "bot_tweets_df.set_index(['user_id',\"timestamp\"],inplace=True)\n",
    "\n",
    "social_tweets_df['timestamp'] = social_tweets_df[\"timestamp\"].apply( lambda social_tweets_df : \n",
    "datetime.datetime(year=social_tweets_df.year, month=social_tweets_df.month, day=social_tweets_df.day))\n",
    "social_tweets_df.set_index(['user_id',\"timestamp\"],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
